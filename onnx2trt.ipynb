{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103103/2640581028.py:17: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 1 << 28\n",
      "/tmp/ipykernel_103103/2640581028.py:18: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  builder.max_batch_size = 1\n",
      "/tmp/ipykernel_103103/2640581028.py:32: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX file from path superpoint.onnx...\n",
      "[07/04/2023-17:41:43] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "Beginning ONNX file parsing\n",
      "[07/04/2023-17:41:43] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "Completed parsing of ONNX file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorrt as trt\n",
    " \n",
    "TRT_LOGGER = trt.Logger()\n",
    "model_path = 'superpoint.onnx'\n",
    "engine_file_path = \"superpoint.trt\"\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)  # batchsize=1\n",
    " \n",
    "with trt.Builder(TRT_LOGGER) as builder, builder.create_network(EXPLICIT_BATCH) \\\n",
    "        as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "    profile = builder.create_optimization_profile()\n",
    "    # FIXME: Hardcoded for ImageNet. The minimum/optimum/maximum dimensions of a dynamic input tensor are the same.\n",
    "    # profile.set_shape(input_tensor_name, (1, 3, 224, 224), (max_batch_size, 3, 224, 224), (max_batch_size, 3, 224, 224))\n",
    "    \n",
    "    config = builder.create_builder_config()\n",
    "    config.add_optimization_profile(profile)\n",
    "    config.max_workspace_size = 1 << 28\n",
    "    builder.max_batch_size = 1\n",
    "    if not os.path.exists(model_path):\n",
    "        print('ONNX file {} not found.'.format(model_path))\n",
    "        exit(0)\n",
    "    print('Loading ONNX file from path {}...'.format(model_path))\n",
    "    with open(model_path, 'rb') as model:\n",
    "        print('Beginning ONNX file parsing')\n",
    "        if not parser.parse(model.read()):\n",
    "            print('ERROR: Failed to parse the ONNX file.')\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    " \n",
    "    network.get_input(0).shape = [1, 1, 800, 1280]\n",
    "    print('Completed parsing of ONNX file')\n",
    "    engine = builder.build_engine(network, config)\n",
    "    with open(engine_file_path, \"wb\") as f:\n",
    "        f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1280)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "from PIL import ImageDraw\n",
    "import sys, os\n",
    "import cv2\n",
    "import torch\n",
    "# sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
    "\n",
    "TRT_LOGGER = trt.Logger()\n",
    "onnx_file_path = \"superpoint.onnx\"\n",
    "engine_file_path = \"superpoint.trt\"\n",
    "img = cv2.imread('img/test.jpg')[:, :, ::-1]\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input = torch.tensor(gray_img, dtype=torch.float, device=device)[None, None] / 255.\n",
    "print(gray_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([493, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "from kornia.feature import SOLD2\n",
    "\n",
    "net = SOLD2(pretrained=True)\n",
    "net = net.to(device)\n",
    "with torch.no_grad():\n",
    "    out = net(input)\n",
    "print(out[\"line_segments\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import common\n",
    "def get_engine(onnx_file_path, engine_file_path=\"\"):\n",
    "    \"\"\"Attempts to load a serialized engine if available, otherwise builds a new TensorRT engine and saves it.\"\"\"\n",
    "\n",
    "    def build_engine():\n",
    "        \"\"\"Takes an ONNX file and creates a TensorRT engine to run inference with\"\"\"\n",
    "        with trt.Builder(TRT_LOGGER) as builder, builder.create_network(\n",
    "            common.EXPLICIT_BATCH\n",
    "        ) as network, builder.create_builder_config() as config, trt.OnnxParser(\n",
    "            network, TRT_LOGGER\n",
    "        ) as parser, trt.Runtime(\n",
    "            TRT_LOGGER\n",
    "        ) as runtime:\n",
    "            config.max_workspace_size = 1 << 28  # 256MiB\n",
    "            builder.max_batch_size = 1\n",
    "            # Parse model file\n",
    "            if not os.path.exists(onnx_file_path):\n",
    "                print(\n",
    "                    \"ONNX file {} not found, please run yolov3_to_onnx.py first to generate it.\".format(onnx_file_path)\n",
    "                )\n",
    "                exit(0)\n",
    "            print(\"Loading ONNX file from path {}...\".format(onnx_file_path))\n",
    "            with open(onnx_file_path, \"rb\") as model:\n",
    "                print(\"Beginning ONNX file parsing\")\n",
    "                if not parser.parse(model.read()):\n",
    "                    print(\"ERROR: Failed to parse the ONNX file.\")\n",
    "                    for error in range(parser.num_errors):\n",
    "                        print(parser.get_error(error))\n",
    "                    return None\n",
    "            # The actual yolov3.onnx is generated with batch size 64. Reshape input to batch size 1\n",
    "            network.get_input(0).shape = [1, 3, 608, 608]\n",
    "            print(\"Completed parsing of ONNX file\")\n",
    "            print(\"Building an engine from file {}; this may take a while...\".format(onnx_file_path))\n",
    "            plan = builder.build_serialized_network(network, config)\n",
    "            engine = runtime.deserialize_cuda_engine(plan)\n",
    "            print(\"Completed creating Engine\")\n",
    "            with open(engine_file_path, \"wb\") as f:\n",
    "                f.write(plan)\n",
    "            return engine\n",
    "\n",
    "    if os.path.exists(engine_file_path):\n",
    "        # If a serialized engine exists, use it instead of building an engine.\n",
    "        print(\"Reading engine from file {}\".format(engine_file_path))\n",
    "        with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            return runtime.deserialize_cuda_engine(f.read())\n",
    "    else:\n",
    "        return build_engine()\n",
    "# Output shapes expected by the post-processor\n",
    "output_shapes = [(1, 255, 19, 19), (1, 255, 38, 38), (1, 255, 76, 76)]\n",
    "# Do inference with TensorRT\n",
    "trt_outputs = []\n",
    "with get_engine(onnx_file_path, engine_file_path) as engine, engine.create_execution_context() as context:\n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n",
    "    # Do inference\n",
    "    print(\"Running inference on image {}...\".format(\"img/test.jpg\"))\n",
    "    # Set host input to the image. The common.do_inference function will copy the input to the GPU before executing.\n",
    "    inputs[0].host = input\n",
    "    trt_outputs = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "    print(trt_outputs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
